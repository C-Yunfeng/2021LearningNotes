##### Java

- [x] array list和linked list的区别

  > - 底层：一个数组，一个链表
  > - 查找：o1，on，链表要从头开始查找

- [ ] hashmap的实现原理【必问】

  > 分为1.7和1.8，现在主要用1.8的版本
  >
  > 根据key进行hash，分配到一个桶的一个位置，就是索引的位置，如果冲突的话，会往下形成一个链表，如果链表长度达到8，会先判断数组长度是不是达到64，如果没到则选择扩容，否则就把链表改成一个红黑树【如果用链表的话，时间复杂度就是on了，如果改成红黑树，可能是logn，相比于一个单链表来说，效率上非常快】

- [ ] java多线程的线程池有几种实现方式，线程池jdk有几种，

  > newCachedThreadPool，newFixedThreadPool，newSingleThreadExecutor

- [ ] JVM的内存划分

  > 就是eden，old。eden里又分老年代和新生代，
  >
  > 有survive区，from区，to区
  >
  > 如果有对象出来之后，先放在新生代区，经过一次垃圾回收，新生代幸存下来转移到from区，下一次再移到to区，采用了复制算法
  >
  > 【标记清除会产生内存碎片，新生代区大部分是可以直接回收掉的，如果用标记清楚会产生大量的内存碎片。新生代的数据少，所以直接采用复制算法，这样来的更快】

- [ ] aop的实现

  > 面向切面编程，切面编程怎么实现的？

- [ ] 一个对象编程多线程线程安全的？

  > 使用的时候加lock，双检索单列

- [ ] concurrent hashmap的原理

  > cas和synchronized，每个进程进来的时候用cas判断一下，是否有人在扩容或者修改这个桶的数据

- [x] 不开辟内存的交换

  > a=a+b
  >
  > b=a-b
  >
  > a=a-b

- [ ] Java 里的final关键用过吗







##### MySQL

- [ ] 主键和索引的搜索速度【innodb】

  主键id，唯一索引userid，二者相等，则select的区别

  > 主键=唯一索引+非空
  >
  > innodb自动再唯一上加索引，长期看一样？
  >
  > 聚簇索引和非聚簇索引？
  >
  > 唯一索引userid有一个回表的过程
  >
  > 【select id from where userid=1，效率会有提升吗？
  >
  > 不会回表，唯一索引B+树叶子结点存的是id，没必要再去查】

- [ ] MySQL表的索引是什么数据结构？

  > B+树，每个节点存储的是索引，叶子结点上存储的是一个真实的数据。其他节点不存数据，可以存更多的数据？叶子结点采用了双向链表的结构，支持sql的范围查找
  >
  > 极端情况下，二叉树可能会退化成像链表一样，on的时间复杂度
  >
  > 平衡二叉树的实现比较复杂，层数限制效率不够好

- [ ] MyISAM和InnoDB的区别

  > M支持事务、外键。I不支持

- [ ] union和union all的区别

  > union去重，union all不去重

- [ ] 聚合函数对空值的态度

  > AVG、SUM忽略空值【不算记录】
  >
  > count(*)不忽略空值，count(1)不忽略空值【二者效率差别也很小】
  >
  > count(col)忽略
  >
  > 

- [ ] 语句执行顺序

  > select放在order前，交换join和on的顺序





##### Spark

- [ ] 任务提交到最终执行是怎样的过程【必问】

  > 代码封装成一个rdd，到一个action真正开始提交，放到一个队列当中，通过无向环形图，把任务切分成一个stage，每个stage从后往前开始切分，用了一个递归，从后往前找，找到第一个之后切分成stage，再从头开始将stage一个个地提交
  >
  > 提交后将stage的task封装成taskset，然后交给taskschedule，sch将taskset放到一个队列当中，等待executor的资源，如果exe准备好了，就会将这些发到executor上执行

  - [ ] stage是基于什么划分的【重点】

    > 根据宽依赖和窄依赖进行划分，每碰到一个宽依赖就划分成一个stage，

- [ ] shuffle的过程

  > 两种，如果分区数比较小，走一个bypass，最终形成两个小文件，一个有index，一个记录数日志。拉取的时候可以通过index知道分区的数据在第几行，就可以直接拉取了
  >
  > 如果数据量很大，走一个sort，排序后形成两个文件，拉取的时候同样通过index下表寻找分区数，







##### 



##### Linux

- [ ] 会awk命令吗
- [ ] 



##### 项目

- [ ] 日志数据采集：flume+kafka+spark+hive+hdfs

  > 



##### 数据结构

- [ ] B和B+
- [ ] 写一个排序算法（插入排序）